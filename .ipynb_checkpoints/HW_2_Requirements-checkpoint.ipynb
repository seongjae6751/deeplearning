{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b96434e-80a8-4d8f-8cf0-a3dc2a743e90",
   "metadata": {},
   "source": [
    "### TitanicDataset 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b6323ba-ddb8-4ac8-984a-dcb17300c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "### 요구사항 2 ####\n",
    "import argparse  # argparse 모듈을 임포트\n",
    "\n",
    "# 나머지 라이브러리 임포트\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # 입력 데이터 X와 레이블 y를 PyTorch 텐서로 변환\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환하는 메서드\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 idx에 해당하는 데이터를 튜플로 반환\n",
    "        feature = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return feature, target  # 튜플 형태로 반환\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋 정보를 문자열로 반환\n",
    "        return \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.X), self.X.shape, self.y.shape\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd4354-02ee-419b-9b7d-b346431ed672",
   "metadata": {},
   "source": [
    "### TitanicTestDataset 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51d53ba1-4c37-4f19-a74a-aa63ab969572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitanicTestDataset 클래스는 테스트 데이터를 위한 Dataset 클래스\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        # 입력 데이터 X를 PyTorch 텐서로 변환\n",
    "        self.X = torch.FloatTensor(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 idx에 해당하는 데이터를 반환\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature}\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋 정보를 문자열로 반환\n",
    "        return \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "            len(self.X), self.X.shape\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44982ed0-b322-4918-899a-9b24206811fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 전처리하고 결합하는 함수\n",
    "def get_preprocessed_dataset():\n",
    "    # 현재 파일 경로를 기준으로 CSV 파일 경로 설정\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    # CSV 파일을 데이터프레임으로 로드\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    # 훈련 데이터와 테스트 데이터를 결합하여 전체 데이터셋 생성\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # 데이터 전처리 단계를 차례대로 적용\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    # 훈련 및 테스트 데이터 분리\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Dataset 생성\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628df664-7c40-43f3-940e-80cbc43bd1db",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af7d2f0d-c7fd-404a-ac35-8cf86b2980b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 1: Pclass별 평균 Fare 값으로 결측치 메우기\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d62b2-f48f-43b6-97e8-4a4f0b74469f",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5d7e6eb-9bb7-4b62-a6e5-b8f7a78b9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 2: 이름(Name)을 세 개의 컬럼으로 분리\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79366a4-7b71-4f71-a22b-d6f4e17af92e",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "650affbe-6a96-475c-995b-cc66f855a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 3: honorific별 Age 결측치 메우기\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\"]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d25b80-a4b6-4c8a-ad4e-b5a1b57c1ffc",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e081f11e-0ef6-4c88-9a4c-5faf2d2b027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 4: 가족수 및 혼자 탑승 여부 파생 변수 추가\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "\n",
    "    # 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d655e-4b92-459d-9585-e261e80af866",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d12e1eb-a02d-4d09-a850-5a5ab29605a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 5: honorific 개수를 줄이고, Embarked 결측치 메우기\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    all_df.loc[~((all_df[\"honorific\"] == \"Mr\") | \n",
    "                 (all_df[\"honorific\"] == \"Miss\") | \n",
    "                 (all_df[\"honorific\"] == \"Mrs\") | \n",
    "                 (all_df[\"honorific\"] == \"Master\")), \"honorific\"] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2cb5c-13d1-4e03-a731-8b5dcacddd13",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0573c93e-fab4-4d5c-9eba-d3e3cb669ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 6: 카테고리형 변수 레이블 인코딩\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "            le = le.fit(all_df[category_feature])\n",
    "            all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f685d-3bbe-453d-8058-1e253c855ec0",
   "metadata": {},
   "source": [
    "### 데이터 로딩 함수(요구사항2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47dbdd4f-2b0e-471d-9725-ced502982340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Titanic 데이터셋 로드\n",
    "    train_dataset, validation_dataset, _ = get_preprocessed_dataset()  # 반환된 세 개의 값 중 세 번째는 사용하지 않음\n",
    "\n",
    "    # Wandb 설정에 따라 배치 크기 지정\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebca49-b44a-4f18-97e0-c51217022e72",
   "metadata": {},
   "source": [
    "### MyModelMyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a7a9f52-7369-4986-a9eb-88e1aeacd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "\n",
    "# # MyModel 클래스는 PyTorch의 nn.Module을 상속받아 모델을 정의합니다.\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, n_input, n_output):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # 모델은 3개의 선형 계층과 활성화 함수 ReLU로 구성\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(n_input, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, n_output),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 입력 데이터를 모델에 전달하여 예측 결과를 반환\n",
    "#         return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5ad06-9126-4a7e-8560-b886ef95697f",
   "metadata": {},
   "source": [
    "### 모델 수정(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06d559e2-02d8-4da8-85e3-2509ffe65f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output, activation_fn):\n",
    "        super().__init__()\n",
    "\n",
    "        if activation_fn == \"ReLU\":\n",
    "            activation = nn.ReLU()\n",
    "        elif activation_fn == \"ELU\":\n",
    "            activation = nn.ELU()\n",
    "        elif activation_fn == \"LeakyReLU\":\n",
    "            activation = nn.LeakyReLU()\n",
    "        elif activation_fn == \"PReLU\":\n",
    "            activation = nn.PReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            activation,\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            activation,\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f0778-7e2f-4462-b9c5-f620bba3993e",
   "metadata": {},
   "source": [
    "### 테스트 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05c9bfa4-e20d-4b42-93b6-4c04af859093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 테스트 데이터를 사용해 모델을 평가하는 함수\n",
    "def test(test_data_loader):\n",
    "    print(\"[TEST]\")\n",
    "    batch = next(iter(test_data_loader))\n",
    "    print(\"{0}\".format(batch['input'].shape))\n",
    "    my_model = MyModel(n_input=11, n_output=2)\n",
    "    my_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_batch = my_model(batch['input'])\n",
    "        prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "\n",
    "        # 예측 결과 출력\n",
    "        for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "            print(idx, prediction.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c957f-b70a-4dab-8018-cad564eb06e5",
   "metadata": {},
   "source": [
    "### 모델 및 Optimizer 설정(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e08f6df-3acc-4de3-9ae4-0e48e0f1b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer():\n",
    "    activation_fn = wandb.config.activation_fn  # 설정된 Activation Function을 가져옴\n",
    "    my_model = MyModel(n_input=11, n_output=2, activation_fn=activation_fn)\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "    return my_model, optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682f695-2854-4567-a218-b14f69276f78",
   "metadata": {},
   "source": [
    "### 훈련 루프(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94b5a899-fb47-420b-aab6-d458fcdd68ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()  # Titanic 데이터는 분류 문제이므로 CrossEntropyLoss 사용\n",
    "    next_print_epoch = 100\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "        for input, target in train_data_loader:\n",
    "            output_train = model(input)\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_validation = 0.0\n",
    "        num_validations = 0\n",
    "        with torch.no_grad():\n",
    "            for input, target in validation_data_loader:\n",
    "                output_validation = model(input)\n",
    "                loss = loss_fn(output_validation, target)\n",
    "                loss_validation += loss.item()\n",
    "                num_validations += 1\n",
    "\n",
    "        # Wandb에 훈련 로그 기록\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": loss_train / num_trains,\n",
    "            \"Validation loss\": loss_validation / num_validations\n",
    "        })\n",
    "\n",
    "        if epoch >= next_print_epoch:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, \"\n",
    "                f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "                f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "            )\n",
    "            next_print_epoch += 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32ea24-bca4-4648-ac4a-df6f2ccbd845",
   "metadata": {},
   "source": [
    "### 각 Activation Function에 대해 학습을 실행할 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef621116-3f14-4f54-8de0-c35669698069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments_with_multiple_activations():\n",
    "    # 테스트할 Activation Function 목록\n",
    "    activation_functions = [\"ReLU\", \"ELU\", \"LeakyReLU\", \"PReLU\"]\n",
    "\n",
    "    # argparse를 한 번 설정하여 인자를 읽어옴\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-b\", \"--batch_size\", type=int, default=16, help=\"Batch size (int, default: 16)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-e\", \"--epochs\", type=int, default=1000, help=\"Number of training epochs (int, default: 1000)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-a\", \"--activation_fn\", type=str, default=\"ReLU\",\n",
    "        choices=[\"ReLU\", \"ELU\", \"LeakyReLU\", \"PReLU\"],\n",
    "        help=\"Activation function to use (default: ReLU)\"\n",
    "    )\n",
    "\n",
    "    # 인자 파싱 (args=[]로 설정하여 Jupyter Notebook에서도 작동하도록)\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # 모든 Activation Function에 대해 반복적으로 학습 수행\n",
    "    for activation_fn in activation_functions:\n",
    "        print(f\"\\nRunning experiment with Activation Function: {activation_fn}\\n\")\n",
    "        # 현재 Activation Function을 설정에 반영\n",
    "        args.activation_fn = activation_fn\n",
    "        main(args)\n",
    "        print(f\"\\nExperiment with {activation_fn} completed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ddcc6-14c4-46a7-adf6-c42a2b6c3ba8",
   "metadata": {},
   "source": [
    "### Main 함수(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46f97c85-1372-4666-877f-9596b2027432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "#     current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "#     # Wandb 설정에서 'batch_size'를 포함했는지 확인\n",
    "#     config = {\n",
    "#         'epochs': args.epochs,\n",
    "#         'batch_size': args.batch_size,\n",
    "#         'learning_rate': 1e-3,\n",
    "#         'n_hidden_unit_list': [20, 20],\n",
    "#         'activation_fn': args.activation_fn  # Activation Function 선택\n",
    "#     }\n",
    "\n",
    "#     # Wandb 초기화\n",
    "#     wandb_run = wandb.init(\n",
    "#         entity=\"seongjae6751\",\n",
    "#         mode=\"online\" if args.wandb else \"disabled\",\n",
    "#         project=\"titanic_model_training\",\n",
    "#         notes=\"Titanic data training experiment\",\n",
    "#         tags=[\"titanic\", \"activation_function\"],\n",
    "#         name=current_time_str,\n",
    "#         config=config  # config 객체를 그대로 사용\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Wandb URL: {wandb_run.url}\")\n",
    "\n",
    "#     # Wandb 객체의 초기화 상태 확인\n",
    "#     if wandb_run is not None:\n",
    "#         print(f\"Wandb Run 초기화 성공: {wandb_run.id}\")\n",
    "#         print(f\"Wandb Mode: {wandb_run.settings.mode}\")\n",
    "#         print(f\"Wandb URL: {wandb_run.url}\")\n",
    "#     else:\n",
    "#         print(\"Wandb 초기화에 실패했습니다.\")\n",
    "\n",
    "#     run_url = wandb.run.url\n",
    "#     print(f\"Wandb URL for {args.activation_fn}: {run_url}\")\n",
    "    \n",
    "#     train_data_loader, validation_data_loader = get_data()\n",
    "#     model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "#     training_loop(\n",
    "#         model=model,\n",
    "#         optimizer=optimizer,\n",
    "#         train_data_loader=train_data_loader,\n",
    "#         validation_data_loader=validation_data_loader\n",
    "#     )\n",
    "#     wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233e2cb-c223-4e6f-974a-6da7cb9196ad",
   "metadata": {},
   "source": [
    "### Main 실행(요구사항2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaba749-65ec-457a-986a-22111c0004b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
