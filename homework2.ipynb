{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b96434e-80a8-4d8f-8cf0-a3dc2a743e90",
   "metadata": {},
   "source": [
    "### TitanicDataset 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7b6323ba-ddb8-4ac8-984a-dcb17300c4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "### 요구사항 2 ####\n",
    "import argparse  # argparse 모듈을 임포트\n",
    "\n",
    "# 나머지 라이브러리 임포트\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from torch import nn, optim\n",
    "\n",
    "\n",
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # 입력 데이터 X와 레이블 y를 PyTorch 텐서로 변환\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환하는 메서드\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 idx에 해당하는 데이터를 튜플로 반환\n",
    "        feature = self.X[idx]\n",
    "        target = self.y[idx]\n",
    "        return feature, target  # 튜플 형태로 반환\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋 정보를 문자열로 반환\n",
    "        return \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "            len(self.X), self.X.shape, self.y.shape\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd4354-02ee-419b-9b7d-b346431ed672",
   "metadata": {},
   "source": [
    "### TitanicTestDataset 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51d53ba1-4c37-4f19-a74a-aa63ab969572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TitanicTestDataset 클래스는 테스트 데이터를 위한 Dataset 클래스\n",
    "class TitanicTestDataset(Dataset):\n",
    "    def __init__(self, X):\n",
    "        # 입력 데이터 X를 PyTorch 텐서로 변환\n",
    "        self.X = torch.FloatTensor(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 크기를 반환\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 인덱스 idx에 해당하는 데이터를 반환\n",
    "        feature = self.X[idx]\n",
    "        return {'input': feature}\n",
    "\n",
    "    def __str__(self):\n",
    "        # 데이터셋 정보를 문자열로 반환\n",
    "        return \"Data Size: {0}, Input Shape: {1}\".format(\n",
    "            len(self.X), self.X.shape\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44982ed0-b322-4918-899a-9b24206811fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 전처리하고 결합하는 함수\n",
    "def get_preprocessed_dataset():\n",
    "    # 현재 파일 경로를 기준으로 CSV 파일 경로 설정\n",
    "    CURRENT_FILE_PATH = os.getcwd()\n",
    "    train_data_path = os.path.join(CURRENT_FILE_PATH, \"train.csv\")\n",
    "    test_data_path = os.path.join(CURRENT_FILE_PATH, \"test.csv\")\n",
    "\n",
    "    # CSV 파일을 데이터프레임으로 로드\n",
    "    train_df = pd.read_csv(train_data_path)\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "    # 훈련 데이터와 테스트 데이터를 결합하여 전체 데이터셋 생성\n",
    "    all_df = pd.concat([train_df, test_df], sort=False)\n",
    "\n",
    "    # 데이터 전처리 단계를 차례대로 적용\n",
    "    all_df = get_preprocessed_dataset_1(all_df)\n",
    "    all_df = get_preprocessed_dataset_2(all_df)\n",
    "    all_df = get_preprocessed_dataset_3(all_df)\n",
    "    all_df = get_preprocessed_dataset_4(all_df)\n",
    "    all_df = get_preprocessed_dataset_5(all_df)\n",
    "    all_df = get_preprocessed_dataset_6(all_df)\n",
    "\n",
    "    # 훈련 및 테스트 데이터 분리\n",
    "    train_X = all_df[~all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "    train_y = train_df[\"Survived\"]\n",
    "    test_X = all_df[all_df[\"Survived\"].isnull()].drop(\"Survived\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Dataset 생성\n",
    "    dataset = TitanicDataset(train_X.values, train_y.values)\n",
    "    train_dataset, validation_dataset = random_split(dataset, [0.8, 0.2])\n",
    "    test_dataset = TitanicTestDataset(test_X.values)\n",
    "\n",
    "    return train_dataset, validation_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628df664-7c40-43f3-940e-80cbc43bd1db",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af7d2f0d-c7fd-404a-ac35-8cf86b2980b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 1: Pclass별 평균 Fare 값으로 결측치 메우기\n",
    "def get_preprocessed_dataset_1(all_df):\n",
    "    Fare_mean = all_df[[\"Pclass\", \"Fare\"]].groupby(\"Pclass\").mean().reset_index()\n",
    "    Fare_mean.columns = [\"Pclass\", \"Fare_mean\"]\n",
    "    all_df = pd.merge(all_df, Fare_mean, on=\"Pclass\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Fare\"].isnull()), \"Fare\"] = all_df[\"Fare_mean\"]\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d62b2-f48f-43b6-97e8-4a4f0b74469f",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5d7e6eb-9bb7-4b62-a6e5-b8f7a78b9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 2: 이름(Name)을 세 개의 컬럼으로 분리\n",
    "def get_preprocessed_dataset_2(all_df):\n",
    "    name_df = all_df[\"Name\"].str.split(\"[,.]\", n=2, expand=True)\n",
    "    name_df.columns = [\"family_name\", \"honorific\", \"name\"]\n",
    "    name_df[\"family_name\"] = name_df[\"family_name\"].str.strip()\n",
    "    name_df[\"honorific\"] = name_df[\"honorific\"].str.strip()\n",
    "    name_df[\"name\"] = name_df[\"name\"].str.strip()\n",
    "    all_df = pd.concat([all_df, name_df], axis=1)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79366a4-7b71-4f71-a22b-d6f4e17af92e",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "650affbe-6a96-475c-995b-cc66f855a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 3: honorific별 Age 결측치 메우기\n",
    "def get_preprocessed_dataset_3(all_df):\n",
    "    honorific_age_mean = all_df[[\"honorific\", \"Age\"]].groupby(\"honorific\").median().round().reset_index()\n",
    "    honorific_age_mean.columns = [\"honorific\", \"honorific_age_mean\"]\n",
    "    all_df = pd.merge(all_df, honorific_age_mean, on=\"honorific\", how=\"left\")\n",
    "    all_df.loc[(all_df[\"Age\"].isnull()), \"Age\"] = all_df[\"honorific_age_mean\"]\n",
    "    all_df = all_df.drop([\"honorific_age_mean\"], axis=1)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d25b80-a4b6-4c8a-ad4e-b5a1b57c1ffc",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e081f11e-0ef6-4c88-9a4c-5faf2d2b027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 4: 가족수 및 혼자 탑승 여부 파생 변수 추가\n",
    "def get_preprocessed_dataset_4(all_df):\n",
    "    all_df[\"family_num\"] = all_df[\"Parch\"] + all_df[\"SibSp\"]\n",
    "    all_df.loc[all_df[\"family_num\"] == 0, \"alone\"] = 1\n",
    "    all_df[\"alone\"].fillna(0, inplace=True)\n",
    "\n",
    "    # 불필요한 컬럼 제거\n",
    "    all_df = all_df.drop([\"PassengerId\", \"Name\", \"family_name\", \"name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2d655e-4b92-459d-9585-e261e80af866",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d12e1eb-a02d-4d09-a850-5a5ab29605a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 5: honorific 개수를 줄이고, Embarked 결측치 메우기\n",
    "def get_preprocessed_dataset_5(all_df):\n",
    "    all_df.loc[~((all_df[\"honorific\"] == \"Mr\") | \n",
    "                 (all_df[\"honorific\"] == \"Miss\") | \n",
    "                 (all_df[\"honorific\"] == \"Mrs\") | \n",
    "                 (all_df[\"honorific\"] == \"Master\")), \"honorific\"] = \"other\"\n",
    "    all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b2cb5c-13d1-4e03-a731-8b5dcacddd13",
   "metadata": {},
   "source": [
    "### get_preprocessed_dataset_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0573c93e-fab4-4d5c-9eba-d3e3cb669ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 단계 6: 카테고리형 변수 레이블 인코딩\n",
    "def get_preprocessed_dataset_6(all_df):\n",
    "    category_features = all_df.columns[all_df.dtypes == \"object\"]\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    for category_feature in category_features:\n",
    "        le = LabelEncoder()\n",
    "        if all_df[category_feature].dtypes == \"object\":\n",
    "            le = le.fit(all_df[category_feature])\n",
    "            all_df[category_feature] = le.transform(all_df[category_feature])\n",
    "\n",
    "    return all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f685d-3bbe-453d-8058-1e253c855ec0",
   "metadata": {},
   "source": [
    "### 데이터 로딩 함수(요구사항2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47dbdd4f-2b0e-471d-9725-ced502982340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Titanic 데이터셋 로드\n",
    "    train_dataset, validation_dataset, _ = get_preprocessed_dataset()  # 반환된 세 개의 값 중 세 번째는 사용하지 않음\n",
    "\n",
    "    # Wandb 설정에 따라 배치 크기 지정\n",
    "    train_data_loader = DataLoader(dataset=train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=len(validation_dataset))\n",
    "\n",
    "    return train_data_loader, validation_data_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ebca49-b44a-4f18-97e0-c51217022e72",
   "metadata": {},
   "source": [
    "### MyModelMyModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a7a9f52-7369-4986-a9eb-88e1aeacd62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import nn\n",
    "\n",
    "# # MyModel 클래스는 PyTorch의 nn.Module을 상속받아 모델을 정의합니다.\n",
    "# class MyModel(nn.Module):\n",
    "#     def __init__(self, n_input, n_output):\n",
    "#         super().__init__()\n",
    "\n",
    "#         # 모델은 3개의 선형 계층과 활성화 함수 ReLU로 구성\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Linear(n_input, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, 30),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(30, n_output),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # 입력 데이터를 모델에 전달하여 예측 결과를 반환\n",
    "#         return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd5ad06-9126-4a7e-8560-b886ef95697f",
   "metadata": {},
   "source": [
    "### 모델 수정(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "06d559e2-02d8-4da8-85e3-2509ffe65f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, n_input, n_output, activation_fn):\n",
    "        super().__init__()\n",
    "\n",
    "        if activation_fn == \"ReLU\":\n",
    "            activation = nn.ReLU()\n",
    "        elif activation_fn == \"ELU\":\n",
    "            activation = nn.ELU()\n",
    "        elif activation_fn == \"LeakyReLU\":\n",
    "            activation = nn.LeakyReLU()\n",
    "        elif activation_fn == \"PReLU\":\n",
    "            activation = nn.PReLU()\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_fn}\")\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_input, wandb.config.n_hidden_unit_list[0]),\n",
    "            activation,\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[0], wandb.config.n_hidden_unit_list[1]),\n",
    "            activation,\n",
    "            nn.Linear(wandb.config.n_hidden_unit_list[1], n_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f0778-7e2f-4462-b9c5-f620bba3993e",
   "metadata": {},
   "source": [
    "### 테스트 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05c9bfa4-e20d-4b42-93b6-4c04af859093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 테스트 데이터를 사용해 모델을 평가하는 함수\n",
    "def test(test_data_loader):\n",
    "    print(\"[TEST]\")\n",
    "    batch = next(iter(test_data_loader))\n",
    "    print(\"{0}\".format(batch['input'].shape))\n",
    "    my_model = MyModel(n_input=11, n_output=2)\n",
    "    my_model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_batch = my_model(batch['input'])\n",
    "        prediction_batch = torch.argmax(output_batch, dim=1)\n",
    "\n",
    "        # 예측 결과 출력\n",
    "        for idx, prediction in enumerate(prediction_batch, start=892):\n",
    "            print(idx, prediction.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681c957f-b70a-4dab-8018-cad564eb06e5",
   "metadata": {},
   "source": [
    "### 모델 및 Optimizer 설정(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7e08f6df-3acc-4de3-9ae4-0e48e0f1b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_optimizer():\n",
    "    activation_fn = wandb.config.activation_fn  # 설정된 Activation Function을 가져옴\n",
    "    my_model = MyModel(n_input=11, n_output=2, activation_fn=activation_fn)\n",
    "    optimizer = optim.SGD(my_model.parameters(), lr=wandb.config.learning_rate)\n",
    "\n",
    "    return my_model, optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682f695-2854-4567-a218-b14f69276f78",
   "metadata": {},
   "source": [
    "### 훈련 루프(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "94b5a899-fb47-420b-aab6-d458fcdd68ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(model, optimizer, train_data_loader, validation_data_loader):\n",
    "    n_epochs = wandb.config.epochs\n",
    "    loss_fn = nn.CrossEntropyLoss()  # Titanic 데이터는 분류 문제이므로 CrossEntropyLoss 사용\n",
    "    next_print_epoch = 100\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        num_trains = 0\n",
    "        for input, target in train_data_loader:\n",
    "            output_train = model(input)\n",
    "            loss = loss_fn(output_train, target)\n",
    "            loss_train += loss.item()\n",
    "            num_trains += 1\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        loss_validation = 0.0\n",
    "        num_validations = 0\n",
    "        with torch.no_grad():\n",
    "            for input, target in validation_data_loader:\n",
    "                output_validation = model(input)\n",
    "                loss = loss_fn(output_validation, target)\n",
    "                loss_validation += loss.item()\n",
    "                num_validations += 1\n",
    "\n",
    "        # Wandb에 훈련 로그 기록\n",
    "        wandb.log({\n",
    "            \"Epoch\": epoch,\n",
    "            \"Training loss\": loss_train / num_trains,\n",
    "            \"Validation loss\": loss_validation / num_validations\n",
    "        })\n",
    "\n",
    "        if epoch >= next_print_epoch:\n",
    "            print(\n",
    "                f\"Epoch {epoch}, \"\n",
    "                f\"Training loss {loss_train / num_trains:.4f}, \"\n",
    "                f\"Validation loss {loss_validation / num_validations:.4f}\"\n",
    "            )\n",
    "            next_print_epoch += 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32ea24-bca4-4648-ac4a-df6f2ccbd845",
   "metadata": {},
   "source": [
    "### 각 Activation Function에 대해 학습을 실행할 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ef621116-3f14-4f54-8de0-c35669698069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments_with_multiple_activations():\n",
    "    # 테스트할 Activation Function 목록\n",
    "    activation_functions = [\"ReLU\", \"ELU\", \"LeakyReLU\", \"PReLU\"]\n",
    "\n",
    "    # argparse를 한 번 설정하여 인자를 읽어옴\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"True or False\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-b\", \"--batch_size\", type=int, default=16, help=\"Batch size (int, default: 16)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-e\", \"--epochs\", type=int, default=1000, help=\"Number of training epochs (int, default: 1000)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-a\", \"--activation_fn\", type=str, default=\"ReLU\",\n",
    "        choices=[\"ReLU\", \"ELU\", \"LeakyReLU\", \"PReLU\"],\n",
    "        help=\"Activation function to use (default: ReLU)\"\n",
    "    )\n",
    "\n",
    "    # 인자 파싱 (args=[]로 설정하여 Jupyter Notebook에서도 작동하도록)\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    # 모든 Activation Function에 대해 반복적으로 학습 수행\n",
    "    for activation_fn in activation_functions:\n",
    "        print(f\"\\nRunning experiment with Activation Function: {activation_fn}\\n\")\n",
    "        # 현재 Activation Function을 설정에 반영\n",
    "        args.activation_fn = activation_fn\n",
    "        main(args)\n",
    "        print(f\"\\nExperiment with {activation_fn} completed!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856ddcc6-14c4-46a7-adf6-c42a2b6c3ba8",
   "metadata": {},
   "source": [
    "### Main 함수(요구사항 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "46f97c85-1372-4666-877f-9596b2027432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(args):\n",
    "#     current_time_str = datetime.now().astimezone().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "#     # Wandb 설정에서 'batch_size'를 포함했는지 확인\n",
    "#     config = {\n",
    "#         'epochs': args.epochs,\n",
    "#         'batch_size': args.batch_size,\n",
    "#         'learning_rate': 1e-3,\n",
    "#         'n_hidden_unit_list': [20, 20],\n",
    "#         'activation_fn': args.activation_fn  # Activation Function 선택\n",
    "#     }\n",
    "\n",
    "#     # Wandb 초기화\n",
    "#     wandb_run = wandb.init(\n",
    "#         entity=\"seongjae6751\",\n",
    "#         mode=\"online\" if args.wandb else \"disabled\",\n",
    "#         project=\"titanic_model_training\",\n",
    "#         notes=\"Titanic data training experiment\",\n",
    "#         tags=[\"titanic\", \"activation_function\"],\n",
    "#         name=current_time_str,\n",
    "#         config=config  # config 객체를 그대로 사용\n",
    "#     )\n",
    "    \n",
    "#     print(f\"Wandb URL: {wandb_run.url}\")\n",
    "\n",
    "#     # Wandb 객체의 초기화 상태 확인\n",
    "#     if wandb_run is not None:\n",
    "#         print(f\"Wandb Run 초기화 성공: {wandb_run.id}\")\n",
    "#         print(f\"Wandb Mode: {wandb_run.settings.mode}\")\n",
    "#         print(f\"Wandb URL: {wandb_run.url}\")\n",
    "#     else:\n",
    "#         print(\"Wandb 초기화에 실패했습니다.\")\n",
    "\n",
    "#     run_url = wandb.run.url\n",
    "#     print(f\"Wandb URL for {args.activation_fn}: {run_url}\")\n",
    "    \n",
    "#     train_data_loader, validation_data_loader = get_data()\n",
    "#     model, optimizer = get_model_and_optimizer()\n",
    "\n",
    "#     training_loop(\n",
    "#         model=model,\n",
    "#         optimizer=optimizer,\n",
    "#         train_data_loader=train_data_loader,\n",
    "#         validation_data_loader=validation_data_loader\n",
    "#     )\n",
    "#     wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233e2cb-c223-4e6f-974a-6da7cb9196ad",
   "metadata": {},
   "source": [
    "### Main 실행(요구사항2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93859aa8-5f1f-4c46-a349-26fe8f3bb3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with Activation Function: ReLU\n",
      "\n",
      "Wandb URL: \n",
      "Wandb Run 초기화 성공: t676c3b6\n",
      "Wandb Mode: \n",
      "Wandb URL: \n",
      "Wandb URL for ReLU: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\391942979.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\277340422.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.5847, Validation loss 0.5395\n",
      "Epoch 200, Training loss 0.5684, Validation loss 0.5494\n",
      "Epoch 300, Training loss 0.5371, Validation loss 0.5031\n",
      "Epoch 400, Training loss 0.5207, Validation loss 0.4881\n",
      "Epoch 500, Training loss 0.5104, Validation loss 0.5109\n",
      "Epoch 600, Training loss 0.4686, Validation loss 0.5633\n",
      "Epoch 700, Training loss 0.5190, Validation loss 0.5568\n",
      "Epoch 800, Training loss 0.4425, Validation loss 0.4095\n",
      "Epoch 900, Training loss 0.4264, Validation loss 0.4419\n",
      "Epoch 1000, Training loss 0.4334, Validation loss 0.4823\n",
      "\n",
      "Experiment with ReLU completed!\n",
      "\n",
      "\n",
      "Running experiment with Activation Function: ELU\n",
      "\n",
      "Wandb URL: \n",
      "Wandb Run 초기화 성공: mwh53nc5\n",
      "Wandb Mode: \n",
      "Wandb URL: \n",
      "Wandb URL for ELU: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\391942979.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\277340422.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.5612, Validation loss 0.6053\n",
      "Epoch 200, Training loss 0.5421, Validation loss 0.5771\n",
      "Epoch 300, Training loss 0.5176, Validation loss 0.6229\n",
      "Epoch 400, Training loss 0.5059, Validation loss 0.5248\n",
      "Epoch 500, Training loss 0.4609, Validation loss 0.5084\n",
      "Epoch 600, Training loss 0.4416, Validation loss 0.5519\n",
      "Epoch 700, Training loss 0.4434, Validation loss 0.5132\n",
      "Epoch 800, Training loss 0.4192, Validation loss 0.5258\n",
      "Epoch 900, Training loss 0.4139, Validation loss 0.6699\n",
      "Epoch 1000, Training loss 0.4248, Validation loss 0.5694\n",
      "\n",
      "Experiment with ELU completed!\n",
      "\n",
      "\n",
      "Running experiment with Activation Function: LeakyReLU\n",
      "\n",
      "Wandb URL: \n",
      "Wandb Run 초기화 성공: 4ekfeytp\n",
      "Wandb Mode: \n",
      "Wandb URL: \n",
      "Wandb URL for LeakyReLU: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\391942979.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\277340422.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.5743, Validation loss 0.5733\n",
      "Epoch 200, Training loss 0.5588, Validation loss 0.5814\n",
      "Epoch 300, Training loss 0.5380, Validation loss 0.5430\n",
      "Epoch 400, Training loss 0.5108, Validation loss 0.4940\n",
      "Epoch 500, Training loss 0.4728, Validation loss 0.4829\n",
      "Epoch 600, Training loss 0.4805, Validation loss 0.4372\n",
      "Epoch 700, Training loss 0.4620, Validation loss 0.4353\n",
      "Epoch 800, Training loss 0.4413, Validation loss 0.4194\n",
      "Epoch 900, Training loss 0.4455, Validation loss 0.4803\n",
      "Epoch 1000, Training loss 0.4473, Validation loss 0.4705\n",
      "\n",
      "Experiment with LeakyReLU completed!\n",
      "\n",
      "\n",
      "Running experiment with Activation Function: PReLU\n",
      "\n",
      "Wandb URL: \n",
      "Wandb Run 초기화 성공: 32v6zxrh\n",
      "Wandb Mode: \n",
      "Wandb URL: \n",
      "Wandb URL for PReLU: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\391942979.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"alone\"].fillna(0, inplace=True)\n",
      "C:\\Users\\seongjae\\AppData\\Local\\Temp\\ipykernel_10752\\277340422.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  all_df[\"Embarked\"].fillna(\"missing\", inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Training loss 0.5775, Validation loss 0.5841\n",
      "Epoch 200, Training loss 0.5717, Validation loss 0.5807\n",
      "Epoch 300, Training loss 0.5581, Validation loss 0.5750\n",
      "Epoch 400, Training loss 0.5344, Validation loss 0.5622\n",
      "Epoch 500, Training loss 0.4973, Validation loss 0.5279\n",
      "Epoch 600, Training loss 0.4830, Validation loss 0.8545\n",
      "Epoch 700, Training loss 0.4406, Validation loss 0.4753\n",
      "Epoch 800, Training loss 0.4242, Validation loss 0.4583\n",
      "Epoch 900, Training loss 0.4231, Validation loss 0.4490\n",
      "Epoch 1000, Training loss 0.4247, Validation loss 0.5423\n",
      "\n",
      "Experiment with PReLU completed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # 데이터셋 전처리 및 로드\n",
    "#     train_dataset, validation_dataset, test_dataset = get_preprocessed_dataset()\n",
    "\n",
    "#     # 데이터셋 정보 출력\n",
    "#     print(\"train_dataset: {0}, validation_dataset.shape: {1}, test_dataset: {2}\".format(\n",
    "#         len(train_dataset), len(validation_dataset), len(test_dataset)\n",
    "#     ))\n",
    "#     print(\"#\" * 50, 1)\n",
    "\n",
    "#     # DataLoader 생성\n",
    "#     train_data_loader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)\n",
    "#     validation_data_loader = DataLoader(dataset=validation_dataset, batch_size=16, shuffle=True)\n",
    "#     test_data_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset))\n",
    "\n",
    "#     # 학습 데이터 출력\n",
    "#     print(\"[TRAIN]\")\n",
    "#     for idx, batch in enumerate(train_data_loader):\n",
    "#         print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "#     # 검증 데이터 출력\n",
    "#     print(\"[VALIDATION]\")\n",
    "#     for idx, batch in enumerate(validation_data_loader):\n",
    "#         print(\"{0} - {1}: {2}\".format(idx, batch['input'].shape, batch['target'].shape))\n",
    "\n",
    "#     # 테스트 데이터 예측\n",
    "#     print(\"#\" * 50, 3)\n",
    "#     test(test_data_loader)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 모든 Activation Function에 대해 실험 수행\n",
    "    run_experiments_with_multiple_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaba749-65ec-457a-986a-22111c0004b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
